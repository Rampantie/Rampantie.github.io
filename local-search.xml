<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>机器学习笔记</title>
    <link href="/2025/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="Part1-Supervised-Machine-Learning-Regression-and-Classification"><a href="#Part1-Supervised-Machine-Learning-Regression-and-Classification" class="headerlink" title="Part1 Supervised Machine Learning Regression and Classification"></a>Part1 Supervised Machine Learning Regression and Classification</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><ul><li>输入到输出 算法，给予正确的输入x和输出y对，使计算机可以正确或相近地预测相应输入后的正确输出。</li><li>分为回归和分类。 </li><li>回归算法：数字预测如从房子面积的大小预测房价（y是连续的）</li><li>分类算法：（y是离散的）</li><li>多个参数进行分类或回归算法</li></ul><h3 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h3><ul><li>集群</li><li>聚类算法：把不同的数据组分到不同的类中，算法自动决定分类依据，将相似的数据组到一组。</li><li>异常检测与降维</li></ul><h3 id="线性回归模式"><a href="#线性回归模式" class="headerlink" title="线性回归模式"></a>线性回归模式</h3><ul><li><p>拟出直线 </p></li><li><p>相关术语：训练集、输入变量&#x2F;特征&#x2F;输入特征（x）、输出变量&#x2F;目标变量（y）、m（训练集的大小）。<br>$$<br>(x,y) :单个训练示例\</p><p>(x^{(i)},y^{(i)}):第i个训练示例\<br>x-&gt;f-&gt;\hat{y}:输入经过学习出的算法计算出预测值\<br>f(x)&#x3D;wx+b:单变量线性回归<br>$$</p></li></ul><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><ul><li><p>代价函数用于衡量一条直线与训练数据的拟合程度</p></li><li><p>平方误差成本函数：线性回归计算中最常用的成本函数</p></li><li><p>计算方式：<br>$$<br>J(w,b) &#x3D; \frac{1}{2m}\sum_{i&#x3D;1}^{m} {(\hat{y}^{(i)}-y^{(i)})^2}<br>$$</p></li><li><p>线性回归的目的是需要找到合适的w,b使J尽量小</p></li><li><p>使J最小的步骤：</p><ul><li>简化情况：让b &#x3D; 0，计算J，现在J只是w的函数</li><li>非简化情况：w,b均会影响J，三维图，汤碗状</li></ul></li></ul><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><ul><li><p>适用于多种成本函数的最小化</p></li><li><p>如同下山一般每次寻找最佳方向直至局部最低点</p></li><li><p>$$<br>repeat{\w &#x3D; w - \alpha \frac{\partial}{\partial{w}}J(w,b)\b &#x3D; b - \alpha \frac{\partial}{\partial b}J(w,b)\}\\alpha:学习率，控制每一步下山的步幅\\alpha过大，摇摆严重；\alpha过小，步骤过多，成本大<br>$$</p></li><li><p>要更新w,b必须同时更新</p></li><li><p>线性回归中的梯度下降算法，只有单一的最小值，碗型凹函数</p></li><li><p>批量梯度下降：每次更新都使用整个训练集</p></li></ul><h3 id="多类特征"><a href="#多类特征" class="headerlink" title="多类特征"></a>多类特征</h3><ul><li><p>上标标识是第几组训练示例，下标标识是第几个特征。</p></li><li><p>$$<br>f(\vec{x})&#x3D;w_{1}x_{1}+w_{2}x_{2}+…+w_{n}x_{n}+b:多元线性回归\\vec{w} &#x3D; [w_{1},w_{2},w_{3},…,w_{n}]\\vec{x} &#x3D; [x_{1},x_{2},x_{3},…,x_{n}]\\vec{w}与b为这个训练模型的参数\f_{\vec{w},b} &#x3D; \vec{w}\cdot\vec{x}+b\n：特征的个数<br>$$</p></li><li><p>矢量化：NumPy库，相较循环速度更快（并行计算），使代码更短更易阅读</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br>f = np.dot(w,x) + b<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">w = np.array([...])<br>d = np.array([...]) (derivatives)<br>w = w - <span class="hljs-number">0.1</span> * d (此时<span class="hljs-number">0.1</span>为学习率)<br></code></pre></td></tr></table></figure><ul><li>多元线性回归中的梯度下降法</li></ul><p>$$<br>repeat{\w_{j} &#x3D; w_{j} - \alpha \frac{\partial}{\partial{w_{j}}}J(\vec{w},b)\b &#x3D; b - \alpha \frac{\partial}{\partial b}J(\vec{w},b)\}<br>$$</p><ul><li>另一种算法适用于线性回归：Normal Equation——无迭代，速度慢</li></ul><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><ul><li><p>数据归一化</p></li><li><p>当特征数值差别较大时，梯度下降算法运行会明显变慢（抖动）；当数据进行适当变换后，速度会加快，这就是特征缩放。</p><img src="/2025/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-10-27%20171524.jpg" class="" title="spaced title"></li><li><p>法一：将数据除以绝对值最大值，归到-1到1的范围内</p><p>法二：均值归一化。归到-1到1。<br>$$<br>x_{i} &#x3D; \frac{x_{i}-\mu_{i}}{x_{max} - x_{min}}<br>$$<br>法三：Z分数归一化。计算平均值和标准差来进行。<br>$$<br>x_{i} &#x3D; \frac{x_{i}-\mu_{i}}{\sigma_{i}}<br>$$</p></li><li><p>学习曲线：成本J关于迭代次数的函数。达到收敛时需要的迭代次数不定。</p><p>选择合适的阈值来确定成本何时收敛。</p></li><li><p>选择优秀的学习率：成本应该随迭代次数增加而不断下降</p><p>某次迭代后成本上升：bug &amp; 学习率过大</p><p>可以先从很小的学习率开始尝试，逐渐增大来选取最佳值，尽量往大了选，可以提高效率。</p></li><li><p>洞察新的变量，改善模型。</p></li><li><p>多项式回归：此时特征缩放很重要，次方的存在会使值扩张很多</p></li></ul><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul><li><p>yes&#x2F;no：二进制分类</p><p>negative&#x2F;positive class</p></li><li><p>logistic回归</p><ul><li><p>sigmoid函数（logistic函数）<br>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}\z &#x3D;\vec{w}\cdot\vec{x}+b\f_{\vec{w},b} &#x3D; g(\vec{w}\cdot\vec{x}+b) &#x3D; \frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}<br>$$<br>outputs between 0 and 1</p><p>![](\机器学习笔记\屏幕截图 2024-10-28 173522.jpg)</p></li></ul></li></ul><p>将线性回归的结果，通过sigmoid函数转换到0-1的范围，实现分类（计算出为1的可能性）。<br>$$<br>f_{\vec{w},b}(\vec{x}) &#x3D; P(y &#x3D; 1|\vec{x};\vec{w},b)<br>$$</p><ul><li><p>决策边界（z &#x3D; 0）:散点图分界线的形状</p><p>选择阈值</p></li></ul><h3 id="logistic回归中的代价函数"><a href="#logistic回归中的代价函数" class="headerlink" title="logistic回归中的代价函数"></a>logistic回归中的代价函数</h3><ul><li>使用之前的平方误差代价函数不再合适，可能会陷入很多局部极小值</li><li>单个训练示例：</li></ul><p>![](\机器学习笔记\屏幕截图 2024-10-29 163533.jpg)</p><p>​整个训练集的成本计算：<br>$$<br>J(w,b) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m} {L(f_{\vec{w},b}(\vec{x}^{(i)}),\vec{y}^{(i)})}<br>$$</p><ul><li>对该代价函数的梯度下降（与上面多元线性回归中的梯度下降法公式相同）：</li></ul><p>$$<br>repeat{\w_{j} &#x3D; w_{j} - \alpha \frac{\partial}{\partial{w_{j}}}J(\vec{w},b)\b &#x3D; b - \alpha \frac{\partial}{\partial b}J(\vec{w},b)\}<br>$$</p><ul><li>学习曲线，特征缩放，矢量化等方法同样可以在logistic回归中应用。</li></ul><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><ul><li><p>欠拟合：偏差较大，不能很好适应训练集(high bias)</p></li><li><p>过拟合：太过符合训练集，不适宜推广到其他例子上，具有高方差(high variance)</p><ul><li>解决方法：；收集更多训练数据；减少特征的数量；<strong>正则化方法</strong></li></ul></li><li><p>正则化：</p><ul><li><p>线性回归中：<br>$$<br>J(\vec{w},b) &#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^{m} ({f_{\vec{w},b}(\vec{x}^{(i)})-\vec{y}^{(i)}})^2+\frac{\lambda}{2m}J(w.b) \sum_{j&#x3D;1}^{n}w_{j}^2\\lambda:正则化参数。过大时，欠拟合；过小时，过拟合。<br>$$<br>​    梯度下降公式等同于前</p></li><li><p>logistic回归中：</p></li></ul></li></ul><p>​<br>$$<br>J(w,b) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m} {L(f_{\vec{w},b}(\vec{x}^{(i)}),\vec{y}^{(i)})}<br>$$<br>​梯度下降公式等同于前</p><h2 id="Part2-Advanced-learning-algorithms"><a href="#Part2-Advanced-learning-algorithms" class="headerlink" title="Part2 Advanced learning algorithms"></a>Part2 Advanced learning algorithms</h2><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><ul><li><p>神经网络：起初用于模仿生物大脑而设计的算法，也被称为深度学习</p><p>语言识别、图像识别、文本识别（自然语言处理）</p></li><li><p>神经网络：接受一些数据，进行计算，输出数据给另一个神经元</p></li><li><p>需求预测：每个神经元进行小小的回归计算，输出a(activations)给另一个神经元</p><p>层(layer)：输入层，隐藏层，输出层</p><p>各个神经元自动选择需要哪些特征数据</p><p>多层神经网络：多层感知器</p></li><li><p>例子：图像识别</p></li></ul><h3 id="神经网络中的层"><a href="#神经网络中的层" class="headerlink" title="神经网络中的层"></a>神经网络中的层</h3><ul><li><p>某层的参数在该字母左上方用方括号标识</p></li><li><p>输入层为第0层</p></li><li><p>逐层计算公式（activation function）：<br>$$<br>a_{j}^{[l]} &#x3D; g(\vec{w}<em>{j}^{[l]}\cdot\vec{a}^{[l-1]}+b</em>{j}^{[l]})\g:sigmoid function<br>$$</p></li><li><p>向前传播（神经网络的预测）：从左到右进行激活值计算、传递。</p></li></ul><h3 id="代码中的推理"><a href="#代码中的推理" class="headerlink" title="代码中的推理"></a>代码中的推理</h3><ul><li><p>常用的神经网络库：TensorFlow,Pytorch</p></li><li><p>TensorFlow中的数据处理，与NumPy中有所不同</p><p>tf.Tensor</p><p>numpy()</p><p>array</p></li><li><p>在TensorFlow中构建神经网络：</p><p>Sequential():将各个层串联在一起</p><p>model.compile(…):编译该模型</p><p>model.fit(x,y):使模型训练，适应训练示例</p><p>model.predict(x_new):用训练好的模型预测新的数据</p></li></ul><h3 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h3><ul><li>单层中的向前传播（代码中的向前传播）</li><li>向前传播的一般实现：Forward prop in NumPy</li><li>大写字母指矩阵，小写字母指向量和标量</li></ul><h3 id="神经网络的矢量化"><a href="#神经网络的矢量化" class="headerlink" title="神经网络的矢量化"></a>神经网络的矢量化</h3><ul><li>np.matmal()：NumPy中矩阵相乘，大幅度加快运算速度</li></ul><h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ul><li>TensorFlow实现：模型定义，编译模型（定义loss function），训练模型（梯度下降，定义梯度下降时迭代的次数）</li></ul><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul><li><p>sigmoid函数的替代：ReLU函数<br>$$<br>g(z) &#x3D; max(0,z)<br>$$<br>激活值现在可以为0或任何非负值</p></li><li><p>选择适合的激活函数（线性激活函数，sigmoid函数，ReLU函数）</p><ul><li>输出层：二进制分类问题使用sigmoid函数；回归问题使用其他函数（看输出值的正负）</li><li>隐藏层：ReLU函数最为常用（速度快，且只在左边部分变得平坦——会影响梯度下降）；不要在隐藏层使用线性激活函数。</li></ul></li><li><p>激活函数的重要性：更复杂的线性回归无法描述的情景</p></li></ul><h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><ul><li><p>y可以接受超过两个值</p></li><li><p>Softmax算法：Logistic回归算法的推广<br>$$<br>z_{i} &#x3D; \vec{w_{i}} \cdot \vec{x} + b_{i}\a_{i} &#x3D; \frac{e^{z_{i}}}{\sum_{j&#x3D;1}^{N}e^{z_{i}}}&#x3D;P(y &#x3D; i|\vec{x})\y可以取N个值；\vec{w_{i}},b_{i}(i&#x3D;1,2,…,N)为这个算法的参数<br>$$<br>损失函数：<br>$$<br>loss(a_{1},a_{2},…a_{N},y)&#x3D;\begin{cases}<br>    -log (a_{1}), &amp; \text{if y &#x3D; 1}\<br>    -log (a_{2}), &amp; \text{if y &#x3D; 2}\<br>    …\<br>    -log (a_{N}), &amp; \text{if y &#x3D; N}\<br>\end{cases}<br>$$</p></li><li><p>神经网络的Softmax输出：最后一层称为Softmax层，有多个输出神经元；每个激活值是所有zi的函数。</p></li><li><p>softmax优化：减少数据舍入时的误差，避免一些过大或过小的计算</p></li><li><p>多标签分类问题：同一个输入进行多个问题的预测</p><p>可以分开解决，或设计一个神经网络同时解决多个问题</p></li></ul><h3 id="高级优化算法"><a href="#高级优化算法" class="headerlink" title="高级优化算法"></a>高级优化算法</h3><ul><li><p>Adam算法：自动调整学习率；对模型的每个参数使用独立的学习率。</p><p>若持续向同样方向靠，增大学习率；若发生震荡，则减小学习率。</p></li></ul><h3 id="更多其它种类的层"><a href="#更多其它种类的层" class="headerlink" title="更多其它种类的层"></a>更多其它种类的层</h3><ul><li><p>卷积层：每个神经元只看前一层输入的某个部分</p><p>计算更快，更少的训练数据需求，更不易过拟合</p><p>参数：窗口大小，神经元个数</p></li></ul><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><ul><li><p>模型出现问题时努力的方向：数据集、参数、模型算法。</p></li><li><p>不要用数据集中的所有数据训练模型，将数据集分为训练集（用于训练）和测试集（用于评估）。</p><p>此时成本函数可以分为训练集成本与测试集成本（不包含正则化参数）。</p><p>分类问题中训练集&#x2F;测试集成本函数可以用不匹配的测试例的比例来表示。</p></li><li><p>测试集成本往往是模型性能更好的指标。</p></li><li><p>更多：将数据集分为训练集、交叉验证集（防止只用测试集导致过于乐观地认为可以很好地推广到新数据）、测试集</p><p><strong>最佳实践：用jtrain优化参数，然后用cv选择了合适的模型，最后用jtest去客观的评估你模型的性能</strong>。提高泛化能力。</p></li></ul><h3 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h3><ul><li>jtrain较高：高偏差，欠拟合；jcv远大于jtrain：方差较高，过拟合</li><li>可能同时出现高方差和高偏差的情况</li><li>正则化参数对算法性能的影响：过小过拟合，过大欠拟合 </li><li>建立表现基准：可以从人类的表现、竞争算法的表现、以前的经验猜测建立评判标准，与jtrain进行比较</li><li>学习曲线：误差随着训练集示例数量mtrain的变化曲线。随着训练集训练示例的数量增加，jtrain逐渐增加，jcv逐渐减小</li></ul><p>​当算法本身偏差bias较大（欠拟合）时，使用再多的训练示例也没有很大的作用在降低jcv上；但过拟合时这样操作是有效果的，可能会低于预期误差。</p><ul><li><p>高方差问题：增加训练集规模，减少特征数量，增大正则化参数</p><p>高偏差问题：增加特征数量，添加额外的多项式特性（升高次数），减小正则化参数</p></li><li><p>偏差&#x2F;方差与神经网络：当神经网络足够大时，训练集中等或更小大小，偏差bias会较低。</p><p>如何校准神经网络？：</p><p>![](\机器学习笔记\屏幕截图 2024-11-12 133148.jpg)</p></li></ul><h3 id="机器学习开发"><a href="#机器学习开发" class="headerlink" title="机器学习开发"></a>机器学习开发</h3><ul><li><p>机器学习开发的迭代循环步骤：选择架构（模型，数据集，等等）——训练模型——诊断（误差分析、方差分析、错误分析）——回到开头，循环往复。</p></li><li><p>错误分析：手动分析错误预测的交叉验证示例，寻找共同特征进行分类，找到问题大头所在。找到该类问题的更多数据，优化模型。对问题出现频率很低的，可以适度忽略。该种方法对于人类擅长的任务有较好的校准作用，对于人类不擅长的效果则不佳。</p></li><li><p>添加数据：添加更多类型的数据，针对预测性能较差的类型添加数据。</p></li><li><p>数据增强：从现有的训练示例创作新的训练示例。在音视频、图像学习上有较大作用。</p></li><li><p>数据合成：大杂烩，逼近现实中的情况。最可能用于计算机视觉工作。</p></li><li><p>AI&#x3D;Code+Data。以模型为中心（传统）、以数据为中心（创新）两种研究模式。</p></li><li><p>迁移学习：预训练好的参数（较大数据集训练，开源）移植到新模型上再进行进一步学习调优参数（较小数据集）。（先监督预学习，再微调参数）自己替换输出层。</p><p>许多视觉识别的早期几层神经网络的作用是一样的，且可迁移的神经网络输入必须是相同的。</p></li><li><p>机器学习项目的完整周期：对项目范围进行划分；收集数据；训练模型；23两步循环往复；生产环境中部署维护，出现问题时及时改进。</p></li><li><p>机器学习模型的部署：客户端、服务端，api通信；监控运行，及时更新。MLOps。</p></li></ul><h3 id="机器学习的伦理问题"><a href="#机器学习的伦理问题" class="headerlink" title="机器学习的伦理问题"></a>机器学习的伦理问题</h3><p>诸多建议。</p><h3 id="倾斜数据集"><a href="#倾斜数据集" class="headerlink" title="倾斜数据集"></a>倾斜数据集</h3><ul><li><p>倾斜数据集：数据集的正负两类出现概率严重不对等。</p></li><li><p>误差测量：此时常用的误差测量方法不能很好的表示模型的性能。</p><p>Confusion Matrix。</p><p>此时使用精确度（预测罕见情况的准确率）和召回率（识别出罕见情况占所有罕见情况的比例）这两个指标来衡量。确保这两个指标足够高。</p></li><li><p>精确度（P）和召回率（R）之间的权衡：选择阈值时考虑。</p><p>在logistic回归中提高预测为1的阈值，慎言，使精确度升高，召回率降低；降低阈值，使精确度降低，召回率升高。 </p></li><li><p>F1 score：<br>$$<br>F_{1} \ score &#x3D; \frac{1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})}<br>$$<br>以一个指标衡量模型。当P或R的任意一个比较小时该指标也会较小。</p></li></ul><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ul><li><p>不同于神经网络。决策树为一步步从根走到叶，在叶结点做出预测。</p></li><li><p>构建决策树的过程，几个关键决定：</p><ol><li>如何决定每个节点使用的特性：纯度最大化。</li><li>什么时候停止分裂：当一个节点百分百是某个类时 或 当进一步拆分会导致树超过提前决定的最大深度（防止树过大、过拟合） 或 分裂一个节点导致的纯度的增加量小于某个阈值收益较低 或 某个节点中的示例个数小于某个阈值。</li></ol></li><li><p>测量某个节点纯度：利用熵来衡量。当两类各占一半时熵最高，为1。当完全为某一类时，熵最低，为0。</p></li><li><p>如何选择每个节点的分裂特征：上级节点的熵减去加权平均左右节点的熵（这个值叫做信息增益），进行比较，谁大选谁（熵减最多，信息增益最大）。当某一步分裂熵减过少，则可以不分裂了。</p></li><li><p>构建决策树的总体过程：从根节点开始，拆分节点，直至停止分裂。在其中0log0视为0。总体是一个递归过程。</p></li><li><p>当特性的选择值不止两个时：使用热编码，使用一般疑问句而非特殊疑问句，创建若干个二进制特性，“这个特征的值是这个吗？”</p><p>热编码技术同样可以用于神经网络的训练。</p></li><li><p>特性的选择值为连续而非离散时的情况：选择该值的划分阈值，选择使信息增益最大的阈值。</p></li><li><p>回归树：决策树在回归问题中的应用（预测一个数字而非类别）</p><p>走到叶节点的实例值得平均值作为预测值。</p><p>如何选择分裂？：使每个节点上的数据方差尽量小。使用左右节点的方差的加权平均来比较哪种分裂方法好，选择较小的。或使用方差减小值。</p></li><li><p>Tree Ensemble（*）：树会对数据小小的变化做出很大的反应。</p><p>使用多棵决策树，再让他们进行投票。</p><ul><li>替换取样：用于构建Tree Ensemble。随机选择训练示例，构造新的训练集，生成一棵树；如此循环往复生成多棵树。</li><li>随机森林算法：使每棵树尽量不同，尽量减小小变化对于整棵树的影响。</li></ul></li><li><p>XGBoost算法&#x2F;Boosted Trees：在后续替换取样时取出已得到决策树识别能力较差的示例。</p></li><li><p><strong>何时使用决策树？</strong>：决策树可以较好地解决结构化数据&#x2F;表格化数据；不建议文本、视频、音频的训练；训练速度快（较神经网络来说）；小决策树基本上是人类可解释、可准确理解的；</p><p><strong>何时使用神经网络？</strong>：适用于所有类型的数据；可以适用于迁移学习；训练速度可能较慢；适用多个机器学习模型协同工作。</p></li></ul><h2 id="Part3-Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning"><a href="#Part3-Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning" class="headerlink" title="Part3 Unsupervised Learning,Recommender Systems and Reinforcement Learning"></a>Part3 Unsupervised Learning,Recommender Systems and Reinforcement Learning</h2><h3 id="无监督学习——聚类"><a href="#无监督学习——聚类" class="headerlink" title="无监督学习——聚类"></a>无监督学习——聚类</h3><ul><li>在数据中寻找特定类型的结构，看看是否可以分组为集群。</li><li>K-means算法：聚类的一种方法。<ol><li>随机初始化k个聚簇中心</li><li>寻找每个点到中心的最短距离，记下该中心的序号</li><li>更新中心为每个中心邻近点集的中心点</li><li>重复2、3两步</li></ol></li><li>优化目标：K-means的算法过程实际上就是在优化某个成本函数（平均的距离平方，失真函数）</li><li>初始化K-means：随机选取k个训练示例，将这k个示例作为初始的k个聚簇中心位置。为防止出现局部极小值而非最小值，最好多次运行K-means算法再找最佳（达到最小成本时的聚簇中心的选择）。</li><li>如何选择聚簇的数量k？：Elbow method，或根据后续的实际需要决定k（推荐）</li></ul><h3 id="无监督学习——异常检测"><a href="#无监督学习——异常检测" class="headerlink" title="无监督学习——异常检测"></a>无监督学习——异常检测</h3><ul><li><p>训练集为大量数据，再判断新数据是正常与否</p></li><li><p>密度估计：阶梯密度等级（为正常的概率），检测是否小于某阈值</p></li><li><p>例子：异常活动检测，数据中心的计算时使用监测</p></li><li><p>绘制数据集的高斯分布（正态分布）：均值、标准差，分布</p></li><li><p>异常检测算法：各分量通过正态分布估计出来概率的乘积</p></li><li><p>开发和评估异常检测系统：交叉验证集和测试集含有少量几个异常的训练示例</p><p>![](\机器学习笔记\屏幕截图 2024-11-18 122827.jpg)</p></li></ul><p>​或不使用测试集，只在交叉验证集上评估。</p><p>​可以使用前面的精确度、召回率、F1 score等指标来评估模型的好坏。</p><ul><li><p>异常检测与监督学习的比较：</p><table><thead><tr><th>异常检测</th><th>监督学习</th></tr></thead><tbody><tr><td>非常少的的positive example（y&#x3D;1）;有许多的出错方式时的训练示例，未来出错时也不清楚异常情况时使用；</td><td>大量的positive example和negative example；异常情况确定；</td></tr></tbody></table></li><li><p>选择要使用的特征：</p><ul><li><p>对于非高斯的特征，将其高斯化（正态化）</p></li><li><p>异常检测系统的错误分析与修正</p><p>如果对于一个p较大但是异常的训练示例，可以考虑是否加入其他特征使其更容易被检测为异常</p></li><li><p>可以组合旧特性（乘积、比值等等运算组合）来形成新特性</p></li></ul></li></ul><h3 id="推荐系统——协同过滤算法"><a href="#推荐系统——协同过滤算法" class="headerlink" title="推荐系统——协同过滤算法"></a>推荐系统——协同过滤算法</h3><ul><li><p>推荐系统含有用户、内容、评价。</p></li><li><p>前提假设：我们知道每个内容的特征，如电影是爱情片还是动作片，有一个概率数字。</p></li><li><p>预测用户对某项内容的评价，生成成本函数（所有已评分的与预测评分之间差距的衡量）。每个用户都要训练出一组参数。（根据内容的特征和用户评分来判断用户偏好）</p></li><li><p>没有每个内容的特征指标的情况：根据每位用户对每项内容的评分和参数，推测每项内容特征指标的值。（根据用户偏好和用户评分来判断内容的特征）</p></li><li><p>协同（从多个用户处收集数据来进行预测）过滤算法：上面两点同时考虑，生成一个成本函数J(w,b,x)，类似线性回归，后面是正则化项</p><p>![](\机器学习笔记\屏幕截图 2024-11-23 113246.jpg)</p></li></ul><p>​最小化这个综合的成本函数可以使用梯度下降算法。</p><ul><li><p>协同过滤算法中的二进制标签情况（上面是评分情况）：例如是否喜欢？是否花时间浏览？是否购买？是否点击？</p><p>用logistic回归来解决，生成预测为1的可能性。</p><p>Loss function.Cost Function.</p></li><li><p>均值归一化：对于未对任何内容或对极少数内容进行评价的用户，如何定义其评价值。将问号定义为该项内容的评价平均值。</p></li><li><p>协同过滤的TensorFlow实现：可以自动计算出成本函数的导数。</p></li><li><p>推荐相关内容的情景：寻找特征指标相近的内容，用特征向量的距离来衡量</p></li><li><p>协同过滤的限制：冷启动问题，“不会给你一个自然的方式来使用附带信息或有关项或用户的其他信息”。</p></li></ul><h3 id="推荐系统——基于内容过滤的算法"><a href="#推荐系统——基于内容过滤的算法" class="headerlink" title="推荐系统——基于内容过滤的算法"></a>推荐系统——基于内容过滤的算法</h3><ul><li><p>需要用户的特性和内容的特性来决定推荐内容，找到好的匹配。</p></li><li><p>该算法中需要用到神经网络架构：将用户特征向量和内容特征向量剪裁</p><p>![](\机器学习笔记\屏幕截图 2024-11-25 111129.jpg)</p></li><li><p>可以借此寻找相似的内容（vm相近）</p></li><li><p>从大目录中选择：进行检索（涵盖有较大可能推荐给用户的内容，再筛选掉一部分不会推荐给用户的内容项）和排名（用神经网络来进行预测评级，将排名列表展示给用户）</p></li><li><p>推荐系统中的伦理问题。</p></li><li><p>在TensorFlow中的实现。</p></li></ul><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul><li><p>应用于机器人控制、工厂优化、金融股票交易、游戏</p></li><li><p>reward function：做好和做坏时给予反馈，让算法自己找出如何选择好的动作</p></li><li><p>状态映射到行动<br>$$<br>(s,a,R(s),s^{‘})<br>\(状态，行动，奖励，新状态)<br>$$</p></li><li><p>discount factor：折现率(γ)<br>$$<br>Return&#x3D;\sum_{j&#x3D;1}^{n}{\gamma^{j-1}R_{j}}<br>$$</p></li><li><p>policy π：强化学习的目标</p><p>what action(a&#x3D;π(s)) to take in every state(s) so as to maximize the return</p></li><li><p>Markov Decision Process：未来只取决于当前状态，而非到达当前状态前的任何事情。强化学习的内核想法。</p><p>![](\机器学习笔记\屏幕截图 2024-11-26 133707.jpg)</p></li><li><p>状态动作价值函数：Optimal Q function(Q*)</p><p>Q(s,a)&#x3D;Return if you </p><ul><li><p>start in state s;</p></li><li><p>take action a (once);</p></li><li><p>then behave optimally after that</p></li></ul><p>在一个状态中选择得到回报最大的动作a:π(s)&#x3D;a。</p></li><li><p>Bellman方程：计算状态动作价值函数的方法<br>$$<br>Q(s,a) &#x3D; R(s) + \gamma\max_{a’} Q(s’,a’)<br>$$<br>s’,a’指下一步的状态和动作</p></li><li><p>随机环境：预期收益最大化（平均的收益回报）<br>$$<br>Q(s,a) &#x3D; R(s) + \gamma E[\max_{a’} Q(s’,a’)]<br>$$</p></li></ul><h3 id="连续状态空间下的强化学习"><a href="#连续状态空间下的强化学习" class="headerlink" title="连续状态空间下的强化学习"></a>连续状态空间下的强化学习</h3><ul><li><p>状态可能包含多个变量，为向量，每个变量均为连续值而非离散。</p></li><li><p>深度强化学习：可以先用监督学习训练出得到Q(s,a)的神经网络。</p></li><li><p>收集第二点中监督学习所说的训练样本。</p><p>![](\机器学习笔记\屏幕截图 2024-11-28 175108.jpg)</p></li><li><p>算法优化：改进的神经网络结构，同时输出各动作的Q(s,a)，再选择最大的那个值。</p></li><li><p>算法优化：ε-贪婪策略，让强化学习系统探索最常用的动作，即使收益并非最大。</p></li><li><p>强化学习对于参数的挑剔程度比监督学习更大。</p></li><li><p>小批处理的思想：可以加快监督学习和强化学习的训练速度。</p><p>在监督学习中，梯度下降时不使用所有训练示例，使用子集。 </p><p>（此时梯度下降的每一步可能不是最好的方向，但最后趋于最小。此时每一小步计算代价会小的多。）</p><p>在强化学习中，每次训练时也只选择一个子集来训练神经网络。</p></li><li><p>软更新：避免某一步中参数变得很糟糕。每次只变动一点点。<br>$$<br>原来：w&#x3D;w_{new}<br>\现在：w&#x3D;0.01w_{new}+0.99w<br>$$</p></li></ul><h3 id="强化学习的限制"><a href="#强化学习的限制" class="headerlink" title="强化学习的限制"></a>强化学习的限制</h3><ul><li>移植到真实环境下的难度较大。</li><li>监督学习更常用，强化学习使用不常见。</li></ul><p><strong>2024.11.29 完结撒花</strong></p><p>![](\机器学习笔记\屏幕截图 2024-11-29 114646.jpg)</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/12/12/hello-world/"/>
    <url>/2025/12/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
